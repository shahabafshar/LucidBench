 {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LucidBench Benchmark Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis of filesystem benchmark results across different storage devices and filesystems.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Loading](#setup)\n",
    "2. [Overview and Summary Statistics](#overview)\n",
    "3. [Performance Analysis by Storage Type](#storage-analysis)\n",
    "4. [Filesystem Comparison](#filesystem-analysis)\n",
    "5. [I/O Pattern Analysis](#io-analysis)\n",
    "6. [Resource Utilization](#resource-analysis)\n",
    "7. [Statistical Analysis](#statistical-analysis)\n",
    "8. [Comparative Analysis](#comparative-analysis)\n",
    "9. [Recommendations and Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_benchmark_data(run_dir):\n",
    "    \"\"\"Load all benchmark and monitoring data from a run directory.\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for test_dir in Path(run_dir).glob('*'):\n",
    "        if not test_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        # Parse test directory name\n",
    "        parts = test_dir.name.split('_')\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "            \n",
    "        storage_type = parts[0]\n",
    "        device = parts[1]\n",
    "        filesystem = parts[2]\n",
    "        test_type = '_'.join(parts[3:])\n",
    "        \n",
    "        # Load test data\n",
    "        test_file = test_dir / 'test.json'\n",
    "        if test_file.exists():\n",
    "            with open(test_file) as f:\n",
    "                test_data = json.load(f)\n",
    "                \n",
    "        # Load monitoring data\n",
    "        monitor_file = test_dir / 'monitoring.json'\n",
    "        if monitor_file.exists():\n",
    "            with open(monitor_file) as f:\n",
    "                monitor_data = json.load(f)\n",
    "                \n",
    "        # Combine data\n",
    "        if 'test_data' in locals() and 'monitor_data' in locals():\n",
    "            data.append({\n",
    "                'storage_type': storage_type,\n",
    "                'device': device,\n",
    "                'filesystem': filesystem,\n",
    "                'test_type': test_type,\n",
    "                'test_data': test_data,\n",
    "                'monitor_data': monitor_data\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load data from the most recent run\n",
    "results_dir = Path('../results')\n",
    "latest_run = max(results_dir.glob('run_*'), key=os.path.getctime)\n",
    "df = load_benchmark_data(latest_run)\n",
    "print(f\"Loaded data from {latest_run.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overview and Summary Statistics <a name=\"overview\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_performance_metrics(row):\n",
    "    \"\"\"Extract key performance metrics from test data.\"\"\"\n",
    "    test_data = row['test_data']\n",
    "    job = test_data['jobs'][0]\n",
    "    \n",
    "    metrics = {\n",
    "        'iops': job['read']['iops'] if 'read' in job else job['write']['iops'],\n",
    "        'bandwidth': job['read']['bw'] if 'read' in job else job['write']['bw'],\n",
    "        'latency': job['read']['lat_ns']['mean'] if 'read' in job else job['write']['lat_ns']['mean'],\n",
    "        'runtime': job['runtime']\n",
    "    }\n",
    "    \n",
    "    return pd.Series(metrics)\n",
    "\n",
    "# Extract performance metrics\n",
    "performance_df = df.apply(extract_performance_metrics, axis=1)\n",
    "df = pd.concat([df, performance_df], axis=1)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary Statistics by Storage Type and Filesystem:\")\n",
    "summary = df.groupby(['storage_type', 'filesystem']).agg({\n",
    "    'iops': ['mean', 'std', 'min', 'max'],\n",
    "    'bandwidth': ['mean', 'std', 'min', 'max'],\n",
    "    'latency': ['mean', 'std', 'min', 'max']\n",
    "}).round(2)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Analysis by Storage Type <a name=\"storage-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_storage_performance(df, metric):\n",
    "    \"\"\"Plot performance metrics by storage type.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create box plot\n",
    "    sns.boxplot(data=df, x='storage_type', y=metric, hue='filesystem')\n",
    "    \n",
    "    plt.title(f'{metric.title()} by Storage Type and Filesystem')\n",
    "    plt.xlabel('Storage Type')\n",
    "    plt.ylabel(metric.title())\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Filesystem')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot performance metrics\n",
    "for metric in ['iops', 'bandwidth', 'latency']:\n",
    "    plot_storage_performance(df, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filesystem Comparison <a name=\"filesystem-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_filesystem_performance(df):\n",
    "    \"\"\"Analyze filesystem performance across different test types.\"\"\"\n",
    "    # Create pivot table for each metric\n",
    "    metrics = ['iops', 'bandwidth', 'latency']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        pivot = pd.pivot_table(\n",
    "            df,\n",
    "            values=metric,\n",
    "            index=['storage_type', 'test_type'],\n",
    "            columns='filesystem',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{metric.upper()} Comparison:\")\n",
    "        display(pivot)\n",
    "        \n",
    "        # Plot comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        pivot.plot(kind='bar')\n",
    "        plt.title(f'{metric.title()} Comparison by Filesystem')\n",
    "        plt.xlabel('Storage Type and Test Type')\n",
    "        plt.ylabel(metric.title())\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(title='Filesystem')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "analyze_filesystem_performance(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. I/O Pattern Analysis <a name=\"io-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_io_patterns(df):\n",
    "    \"\"\"Analyze I/O patterns across different test types.\"\"\"\n",
    "    # Group by test type and calculate statistics\n",
    "    io_stats = df.groupby('test_type').agg({\n",
    "        'iops': ['mean', 'std', 'min', 'max'],\n",
    "        'bandwidth': ['mean', 'std', 'min', 'max'],\n",
    "        'latency': ['mean', 'std', 'min', 'max']\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nI/O Pattern Statistics:\")\n",
    "    display(io_stats)\n",
    "    \n",
    "    # Plot I/O patterns\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # IOPS by test type\n",
    "    sns.boxplot(data=df, x='test_type', y='iops', ax=axes[0,0])\n",
    "    axes[0,0].set_title('IOPS by Test Type')\n",
    "    axes[0,0].set_xticklabels(axes[0,0].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # Bandwidth by test type\n",
    "    sns.boxplot(data=df, x='test_type', y='bandwidth', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Bandwidth by Test Type')\n",
    "    axes[0,1].set_xticklabels(axes[0,1].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # Latency by test type\n",
    "    sns.boxplot(data=df, x='test_type', y='latency', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Latency by Test Type')\n",
    "    axes[1,0].set_xticklabels(axes[1,0].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # IOPS vs Bandwidth scatter\n",
    "    sns.scatterplot(data=df, x='iops', y='bandwidth', hue='test_type', ax=axes[1,1])\n",
    "    axes[1,1].set_title('IOPS vs Bandwidth')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_io_patterns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resource Utilization <a name=\"resource-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_resource_utilization(df):\n",
    "    \"\"\"Analyze system resource utilization during benchmarks.\"\"\"\n",
    "    # Extract monitoring data\n",
    "    monitor_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        for stat in row['monitor_data']['stats']:\n",
    "            monitor_data.append({\n",
    "                'storage_type': row['storage_type'],\n",
    "                'filesystem': row['filesystem'],\n",
    "                'test_type': row['test_type'],\n",
    "                'timestamp': stat['timestamp'],\n",
    "                'cpu_percent': stat['cpu_percent'],\n",
    "                'memory_percent': stat['memory_percent'],\n",
    "                'disk_read_bytes': stat['disk_read_bytes'],\n",
    "                'disk_write_bytes': stat['disk_write_bytes']\n",
    "            })\n",
    "    \n",
    "    monitor_df = pd.DataFrame(monitor_data)\n",
    "    \n",
    "    # Plot resource utilization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # CPU utilization\n",
    "    sns.boxplot(data=monitor_df, x='storage_type', y='cpu_percent', hue='filesystem', ax=axes[0,0])\n",
    "    axes[0,0].set_title('CPU Utilization by Storage Type')\n",
    "    \n",
    "    # Memory utilization\n",
    "    sns.boxplot(data=monitor_df, x='storage_type', y='memory_percent', hue='filesystem', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Memory Utilization by Storage Type')\n",
    "    \n",
    "    # Disk read bytes\n",
    "    sns.boxplot(data=monitor_df, x='storage_type', y='disk_read_bytes', hue='filesystem', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Disk Read Bytes by Storage Type')\n",
    "    \n",
    "    # Disk write bytes\n",
    "    sns.boxplot(data=monitor_df, x='storage_type', y='disk_write_bytes', hue='filesystem', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Disk Write Bytes by Storage Type')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_resource_utilization(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Analysis <a name=\"statistical-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def perform_statistical_analysis(df):\n",
    "    \"\"\"Perform statistical analysis on benchmark results.\"\"\"\n",
    "    # ANOVA test for each metric\n",
    "    metrics = ['iops', 'bandwidth', 'latency']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        print(f\"\\nANOVA Test for {metric.upper()}:\")\n",
    "        \n",
    "        # Test by storage type\n",
    "        storage_groups = [group for _, group in df.groupby('storage_type')[metric]]\n",
    "        f_stat, p_val = stats.f_oneway(*storage_groups)\n",
    "        print(f\"Storage Type ANOVA: F-statistic = {f_stat:.2f}, p-value = {p_val:.4f}\")\n",
    "        \n",
    "        # Test by filesystem\n",
    "        fs_groups = [group for _, group in df.groupby('filesystem')[metric]]\n",
    "        f_stat, p_val = stats.f_oneway(*fs_groups)\n",
    "        print(f\"Filesystem ANOVA: F-statistic = {f_stat:.2f}, p-value = {p_val:.4f}\")\n",
    "        \n",
    "        # Correlation analysis\n",
    "        corr_matrix = df[metrics].corr()\n",
    "        print(f\"\\nCorrelation Matrix for {metric}:\")\n",
    "        display(corr_matrix)\n",
    "        \n",
    "        # Plot correlation heatmap\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title(f'Correlation Matrix for {metric}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "perform_statistical_analysis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparative Analysis <a name=\"comparative-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def perform_comparative_analysis(df):\n",
    "    \"\"\"Perform comparative analysis between different configurations.\"\"\"\n",
    "    # Create comparison matrix\n",
    "    metrics = ['iops', 'bandwidth', 'latency']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        # Compare storage types\n",
    "        storage_comparison = pd.pivot_table(\n",
    "            df,\n",
    "            values=metric,\n",
    "            index='test_type',\n",
    "            columns='storage_type',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{metric.upper()} Comparison by Storage Type:\")\n",
    "        display(storage_comparison)\n",
    "        \n",
    "        # Compare filesystems\n",
    "        fs_comparison = pd.pivot_table(\n",
    "            df,\n",
    "            values=metric,\n",
    "            index='test_type',\n",
    "            columns='filesystem',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{metric.upper()} Comparison by Filesystem:\")\n",
    "        display(fs_comparison)\n",
    "        \n",
    "        # Plot comparison heatmaps\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        sns.heatmap(storage_comparison, annot=True, cmap='YlOrRd', ax=ax1)\n",
    "        ax1.set_title(f'{metric.upper()} by Storage Type')\n",
    "        \n",
    "        sns.heatmap(fs_comparison, annot=True, cmap='YlOrRd', ax=ax2)\n",
    "        ax2.set_title(f'{metric.upper()} by Filesystem')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "perform_comparative_analysis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Recommendations and Conclusions <a name=\"conclusions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_recommendations(df):\n",
    "    \"\"\"Generate recommendations based on benchmark results.\"\"\"\n",
    "    # Calculate performance scores\n",
    "    df['performance_score'] = (\n",
    "        df['iops'] / df['iops'].max() +\n",
    "        df['bandwidth'] / df['bandwidth'].max() -\n",
    "        df['latency'] / df['latency'].max()\n",
    "    ) / 3\n",
    "    \n",
    "    # Group by storage type and filesystem\n",
    "    recommendations = df.groupby(['storage_type', 'filesystem']).agg({\n",
    "        'performance_score': 'mean',\n",
    "        'iops': 'mean',\n",
    "        'bandwidth': 'mean',\n",
    "        'latency': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nPerformance Scores and Recommendations:\")\n",
    "    display(recommendations)\n",
    "    \n",
    "    # Plot performance scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=df, x='storage_type', y='performance_score', hue='filesystem')\n",
    "    plt.title('Overall Performance Score by Storage Type and Filesystem')\n",
    "    plt.xlabel('Storage Type')\n",
    "    plt.ylabel('Performance Score')\n",
    "    plt.legend(title='Filesystem')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "generate_recommendations(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}