{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LucidBench Benchmark Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis of filesystem benchmark results across different storage devices and filesystems.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Loading](#setup)\n",
    "2. [Overview and Summary Statistics](#overview)\n",
    "3. [Performance Analysis by Storage Type](#storage-analysis)\n",
    "4. [Filesystem Comparison](#filesystem-analysis)\n",
    "5. [I/O Pattern Analysis](#io-analysis)\n",
    "6. [Resource Utilization](#resource-analysis)\n",
    "7. [Statistical Analysis](#statistical-analysis)\n",
    "8. [Comparative Analysis](#comparative-analysis)\n",
    "9. [Recommendations and Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from run_20250427_084217\n"
     ]
    }
   ],
   "source": [
    "def load_benchmark_data(run_dir):\n",
    "    \"\"\"Load all benchmark and monitoring data from a run directory.\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for test_dir in Path(run_dir).glob('*'):\n",
    "        if not test_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        # Parse test directory name\n",
    "        parts = test_dir.name.split('_')\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "            \n",
    "        storage_type = parts[0]\n",
    "        device = parts[1]\n",
    "        filesystem = parts[2]\n",
    "        test_type = '_'.join(parts[3:])\n",
    "        \n",
    "        # Load test data\n",
    "        test_file = test_dir / 'test.json'\n",
    "        if test_file.exists():\n",
    "            with open(test_file) as f:\n",
    "                test_data = json.load(f)\n",
    "                \n",
    "        # Load monitoring data\n",
    "        monitor_file = test_dir / 'monitoring.json'\n",
    "        if monitor_file.exists():\n",
    "            with open(monitor_file) as f:\n",
    "                monitor_data = json.load(f)\n",
    "                \n",
    "        # Combine data\n",
    "        if 'test_data' in locals() and 'monitor_data' in locals():\n",
    "            data.append({\n",
    "                'storage_type': storage_type,\n",
    "                'device': device,\n",
    "                'filesystem': filesystem,\n",
    "                'test_type': test_type,\n",
    "                'test_data': test_data,\n",
    "                'monitor_data': monitor_data\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load data from the most recent run\n",
    "results_dir = Path('../results')\n",
    "latest_run = max(results_dir.glob('run_*'), key=os.path.getctime)\n",
    "df = load_benchmark_data(latest_run)\n",
    "print(f\"Loaded data from {latest_run.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overview and Summary Statistics <a name=\"overview\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'runtime'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.Series(metrics)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Extract performance metrics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m performance_df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_performance_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m df = pd.concat([df, performance_df], axis=\u001b[32m1\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Display summary statistics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LucidBench/venv/lib/python3.12/site-packages/pandas/core/frame.py:10374\u001b[39m, in \u001b[36mDataFrame.apply\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10360\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[32m  10362\u001b[39m op = frame_apply(\n\u001b[32m  10363\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10364\u001b[39m     func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10372\u001b[39m     kwargs=kwargs,\n\u001b[32m  10373\u001b[39m )\n\u001b[32m> \u001b[39m\u001b[32m10374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LucidBench/venv/lib/python3.12/site-packages/pandas/core/apply.py:916\u001b[39m, in \u001b[36mFrameApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_raw(engine=\u001b[38;5;28mself\u001b[39m.engine, engine_kwargs=\u001b[38;5;28mself\u001b[39m.engine_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LucidBench/venv/lib/python3.12/site-packages/pandas/core/apply.py:1063\u001b[39m, in \u001b[36mFrameApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m         results, res_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m         results, res_index = \u001b[38;5;28mself\u001b[39m.apply_series_numba()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LucidBench/venv/lib/python3.12/site-packages/pandas/core/apply.py:1081\u001b[39m, in \u001b[36mFrameApply.apply_series_generator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[32m   1080\u001b[39m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m         results[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[32m   1083\u001b[39m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[32m   1084\u001b[39m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[32m   1085\u001b[39m             results[i] = results[i].copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mextract_performance_metrics\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m      3\u001b[39m test_data = row[\u001b[33m'\u001b[39m\u001b[33mtest_data\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m job = test_data[\u001b[33m'\u001b[39m\u001b[33mjobs\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m      6\u001b[39m metrics = {\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33miops\u001b[39m\u001b[33m'\u001b[39m: job[\u001b[33m'\u001b[39m\u001b[33mread\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33miops\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mread\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m job \u001b[38;5;28;01melse\u001b[39;00m job[\u001b[33m'\u001b[39m\u001b[33mwrite\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33miops\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbandwidth\u001b[39m\u001b[33m'\u001b[39m: job[\u001b[33m'\u001b[39m\u001b[33mread\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mbw\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mread\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m job \u001b[38;5;28;01melse\u001b[39;00m job[\u001b[33m'\u001b[39m\u001b[33mwrite\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mbw\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlatency\u001b[39m\u001b[33m'\u001b[39m: job[\u001b[33m'\u001b[39m\u001b[33mread\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlat_ns\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mread\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m job \u001b[38;5;28;01melse\u001b[39;00m job[\u001b[33m'\u001b[39m\u001b[33mwrite\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlat_ns\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mruntime\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mjob\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mruntime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     11\u001b[39m }\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.Series(metrics)\n",
      "\u001b[31mKeyError\u001b[39m: 'runtime'"
     ]
    }
   ],
   "source": [
    "def extract_performance_metrics(row):\n",
    "    \"\"\"Extract key performance metrics from test data.\"\"\"\n",
    "    test_data = row['test_data']\n",
    "    job = test_data['jobs'][0]\n",
    "    \n",
    "    metrics = {\n",
    "        'iops': job['read']['iops'] if 'read' in job else job['write']['iops'],\n",
    "        'bandwidth': job['read']['bw'] if 'read' in job else job['write']['bw'],\n",
    "        'latency': job['read']['lat_ns']['mean'] if 'read' in job else job['write']['lat_ns']['mean'],\n",
    "        'runtime': job.get('runtime', None)\n",
    "    }\n",
    "    \n",
    "    return pd.Series(metrics)\n",
    "\n",
    "# Extract performance metrics\n",
    "performance_df = df.apply(extract_performance_metrics, axis=1)\n",
    "df = pd.concat([df, performance_df], axis=1)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary Statistics by Storage Type and Filesystem:\")\n",
    "summary = df.groupby(['storage_type', 'filesystem']).agg({\n",
    "    'iops': ['mean', 'std', 'min', 'max'],\n",
    "    'bandwidth': ['mean', 'std', 'min', 'max'],\n",
    "    'latency': ['mean', 'std', 'min', 'max']\n",
    "}).round(2)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Analysis by Storage Type <a name=\"storage-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_storage_performance(df, metric):\n",
    "    \"\"\"Plot performance metrics by storage type.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create box plot\n",
    "    sns.boxplot(data=df, x='storage_type', y=metric, hue='filesystem')\n",
    "    \n",
    "    plt.title(f'{metric.title()} by Storage Type and Filesystem')\n",
    "    plt.xlabel('Storage Type')\n",
    "    plt.ylabel(metric.title())\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Filesystem')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot performance metrics\n",
    "for metric in ['iops', 'bandwidth', 'latency']:\n",
    "    plot_storage_performance(df, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filesystem Comparison <a name=\"filesystem-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_filesystem_performance(df):\n",
    "    \"\"\"Analyze filesystem performance across different test types.\"\"\"\n",
    "    # Create pivot table for each metric\n",
    "    metrics = ['iops', 'bandwidth', 'latency']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        pivot = pd.pivot_table(\n",
    "            df,\n",
    "            values=metric,\n",
    "            index=['storage_type', 'test_type'],\n",
    "            columns='filesystem',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{metric.upper()} Comparison:\")\n",
    "        display(pivot)\n",
    "        \n",
    "        # Plot comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        pivot.plot(kind='bar')\n",
    "        plt.title(f'{metric.title()} Comparison by Filesystem')\n",
    "        plt.xlabel('Storage Type and Test Type')\n",
    "        plt.ylabel(metric.title())\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(title='Filesystem')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "analyze_filesystem_performance(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. I/O Pattern Analysis <a name=\"io-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_io_patterns(df):\n",
    "    \"\"\"Analyze I/O patterns across different test types.\"\"\"\n",
    "    # Group by test type and calculate statistics\n",
    "    io_stats = df.groupby('test_type').agg({\n",
    "        'iops': ['mean', 'std', 'min', 'max'],\n",
    "        'bandwidth': ['mean', 'std', 'min', 'max'],\n",
    "        'latency': ['mean', 'std', 'min', 'max']\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nI/O Pattern Statistics:\")\n",
    "    display(io_stats)\n",
    "    \n",
    "    # Plot I/O patterns\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # IOPS by test type\n",
    "    sns.boxplot(data=df, x='test_type', y='iops', ax=axes[0,0])\n",
    "    axes[0,0].set_title('IOPS by Test Type')\n",
    "    axes[0,0].set_xticklabels(axes[0,0].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # Bandwidth by test type\n",
    "    sns.boxplot(data=df, x='test_type', y='bandwidth', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Bandwidth by Test Type')\n",
    "    axes[0,1].set_xticklabels(axes[0,1].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # Latency by test type\n",
    "    sns.boxplot(data=df, x='test_type', y='latency', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Latency by Test Type')\n",
    "    axes[1,0].set_xticklabels(axes[1,0].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # IOPS vs Bandwidth scatter\n",
    "    sns.scatterplot(data=df, x='iops', y='bandwidth', hue='test_type', ax=axes[1,1])\n",
    "    axes[1,1].set_title('IOPS vs Bandwidth')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_io_patterns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resource Utilization <a name=\"resource-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_resource_utilization(df):\n",
    "    \"\"\"Analyze system resource utilization during benchmarks.\"\"\"\n",
    "    # Extract monitoring data\n",
    "    monitor_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        for stat in row['monitor_data']['stats']:\n",
    "            monitor_data.append({\n",
    "                'storage_type': row['storage_type'],\n",
    "                'filesystem': row['filesystem'],\n",
    "                'test_type': row['test_type'],\n",
    "                'timestamp': stat['timestamp'],\n",
    "                'cpu_percent': stat['cpu_percent'],\n",
    "                'memory_percent': stat['memory_percent'],\n",
    "                'disk_read_bytes': stat['disk_read_bytes'],\n",
    "                'disk_write_bytes': stat['disk_write_bytes']\n",
    "            })\n",
    "    \n",
    "    monitor_df = pd.DataFrame(monitor_data)\n",
    "    \n",
    "    # Plot resource utilization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # CPU utilization\n",
    "    sns.boxplot(data=monitor_df, x='storage_type', y='cpu_percent', hue='filesystem', ax=axes[0,0])\n",
    "    axes[0,0].set_title('CPU Utilization by Storage Type')\n",
    "    \n",
    "    # Memory utilization\n",
    "    sns.boxplot(data=monitor_df, x='storage_type', y='memory_percent', hue='filesystem', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Memory Utilization by Storage Type')\n",
    "    \n",
    "    # Disk read bytes\n",
    "    sns.boxplot(data=monitor_df, x='storage_type', y='disk_read_bytes', hue='filesystem', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Disk Read Bytes by Storage Type')\n",
    "    \n",
    "    # Disk write bytes\n",
    "    sns.boxplot(data=monitor_df, x='storage_type', y='disk_write_bytes', hue='filesystem', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Disk Write Bytes by Storage Type')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_resource_utilization(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Analysis <a name=\"statistical-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_statistical_analysis(df):\n",
    "    \"\"\"Perform statistical analysis on benchmark results.\"\"\"\n",
    "    # ANOVA test for each metric\n",
    "    metrics = ['iops', 'bandwidth', 'latency']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        print(f\"\\nANOVA Test for {metric.upper()}:\")\n",
    "        \n",
    "        # Test by storage type\n",
    "        storage_groups = [group for _, group in df.groupby('storage_type')[metric]]\n",
    "        f_stat, p_val = stats.f_oneway(*storage_groups)\n",
    "        print(f\"Storage Type ANOVA: F-statistic = {f_stat:.2f}, p-value = {p_val:.4f}\")\n",
    "        \n",
    "        # Test by filesystem\n",
    "        fs_groups = [group for _, group in df.groupby('filesystem')[metric]]\n",
    "        f_stat, p_val = stats.f_oneway(*fs_groups)\n",
    "        print(f\"Filesystem ANOVA: F-statistic = {f_stat:.2f}, p-value = {p_val:.4f}\")\n",
    "        \n",
    "        # Correlation analysis\n",
    "        corr_matrix = df[metrics].corr()\n",
    "        print(f\"\\nCorrelation Matrix for {metric}:\")\n",
    "        display(corr_matrix)\n",
    "        \n",
    "        # Plot correlation heatmap\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title(f'Correlation Matrix for {metric}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "perform_statistical_analysis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparative Analysis <a name=\"comparative-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_comparative_analysis(df):\n",
    "    \"\"\"Perform comparative analysis between different configurations.\"\"\"\n",
    "    # Create comparison matrix\n",
    "    metrics = ['iops', 'bandwidth', 'latency']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        # Compare storage types\n",
    "        storage_comparison = pd.pivot_table(\n",
    "            df,\n",
    "            values=metric,\n",
    "            index='test_type',\n",
    "            columns='storage_type',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{metric.upper()} Comparison by Storage Type:\")\n",
    "        display(storage_comparison)\n",
    "        \n",
    "        # Compare filesystems\n",
    "        fs_comparison = pd.pivot_table(\n",
    "            df,\n",
    "            values=metric,\n",
    "            index='test_type',\n",
    "            columns='filesystem',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{metric.upper()} Comparison by Filesystem:\")\n",
    "        display(fs_comparison)\n",
    "        \n",
    "        # Plot comparison heatmaps\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        sns.heatmap(storage_comparison, annot=True, cmap='YlOrRd', ax=ax1)\n",
    "        ax1.set_title(f'{metric.upper()} by Storage Type')\n",
    "        \n",
    "        sns.heatmap(fs_comparison, annot=True, cmap='YlOrRd', ax=ax2)\n",
    "        ax2.set_title(f'{metric.upper()} by Filesystem')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "perform_comparative_analysis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Recommendations and Conclusions <a name=\"conclusions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(df):\n",
    "    \"\"\"Generate recommendations based on benchmark results.\"\"\"\n",
    "    # Calculate performance scores\n",
    "    df['performance_score'] = (\n",
    "        df['iops'] / df['iops'].max() +\n",
    "        df['bandwidth'] / df['bandwidth'].max() -\n",
    "        df['latency'] / df['latency'].max()\n",
    "    ) / 3\n",
    "    \n",
    "    # Group by storage type and filesystem\n",
    "    recommendations = df.groupby(['storage_type', 'filesystem']).agg({\n",
    "        'performance_score': 'mean',\n",
    "        'iops': 'mean',\n",
    "        'bandwidth': 'mean',\n",
    "        'latency': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nPerformance Scores and Recommendations:\")\n",
    "    display(recommendations)\n",
    "    \n",
    "    # Plot performance scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=df, x='storage_type', y='performance_score', hue='filesystem')\n",
    "    plt.title('Overall Performance Score by Storage Type and Filesystem')\n",
    "    plt.xlabel('Storage Type')\n",
    "    plt.ylabel('Performance Score')\n",
    "    plt.legend(title='Filesystem')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "generate_recommendations(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
